# Methods

## 1. Data Source and Collection

We downloaded DFT-calculated structure and property data for double perovskites (ABC₂D₆ family) from the Materials Project database using the next-generation API (mp-api v0.41.0+) on October 11, 2025. The Materials Project is a comprehensive database of computed materials properties based on density functional theory (DFT) calculations.

**Query Parameters:**
- Chemical formula pattern: ABC₂D₆ (double perovskites)
- Number of elements: 4-6
- Band gap range: 0.0 - 10.0 eV
- Energy above hull: ≤ 0.2 eV/atom (thermodynamically favorable materials)
- Number of sites: 1-200

The complete list of Materials Project IDs and query parameters are provided in Supplementary Table S1.

## 2. DFT Calculation Details

All DFT data were generated by the Materials Project consortium using the Vienna Ab initio Simulation Package (VASP). Calculations employed:
- **Exchange-correlation functional:** GGA-PBE or r²SCAN functionals
- **Pseudopotentials:** PAW pseudopotentials
- **k-point density:** Automated convergence testing
- **Energy cutoff:** Material-dependent, typically 520 eV

We relied on the Materials Project's reported band gap values and relaxed crystal structures. It is well-established that DFT-GGA calculations underestimate band gaps compared to experimental values, typically by 30-50%. This is an inherent limitation of the training data, not the ML models.

## 3. Dataset Preparation

### 3.1 Data Filtering
- Removed materials without band gap data
- Removed duplicate entries based on chemical formula and space group
- Created two dataset variants:
  - **Dataset A:** All double perovskites (N = 5,776 materials)
  - **Dataset B:** Non-metallic materials only (band gap ≥ 0.1 eV, N = 4,663 materials - indirect bandgap subset)

### 3.2 Missing Data
Missing values in feature vectors were handled using multiple imputation strategies:
- **Mean imputation:** Replace missing values with column mean
- **Median imputation:** Replace with column median
- **KNN imputation:** K-nearest neighbors (k=5)
- **MICE:** Multiple Imputation by Chained Equations (10 iterations)
- **Zero-fill:** Replace with zeros (for PCA-based approaches)

## 4. Feature Engineering

Following the approach of Sradhasagar et al. (2024), we generated approximately 300 compositional and structural descriptors using the matminer library (v0.9.0+) and pymatgen (v2023.9.0+).

### 4.1 Elemental Features
Using matminer's `ElementProperty.from_preset('magpie')`:
- **Stoichiometric weighted averages** of elemental properties:
  - Atomic number (Z)
  - Atomic mass
  - Electronegativity (Pauling scale)
  - Ionization energy
  - Electron affinity
  - Atomic radius (covalent)
  - Number of valence electrons
  - Number of d-electrons
  - Group number and period

For each property P, we computed:
- Mean, standard deviation, minimum, maximum
- Range (max - min)
- Difference metrics

### 4.2 Structural Features
When lattice parameters were available:
- Lattice constants: a, b, c (Å)
- Lattice angles: α, β, γ (degrees)
- Unit cell volume (Ų)
- Density (g/cm³)
- Number of sites in unit cell
- Space group number and symbol
- Derived features: lattice ratios (b/a, c/a), angular deviations from cubic (|α-90°|, etc.)

### 4.3 Derived Compositional Features
- Electronegativity differences between constituent elements
- Mass-to-atomic-number ratios
- Fractional composition of metals vs. non-metals
- Formation energy per atom (from DFT)
- Energy above convex hull

The complete feature list with descriptions is provided in Supplementary Table S2.

## 5. Data Preprocessing

### 5.1 Scaling
All features were scaled using **RobustScaler** (median and IQR-based scaling) to handle outliers better than standard normalization.

### 5.2 Train-Test Split
- **Split ratio:** 80% training, 20% test
- **Random seed:** 42 (for reproducibility)
- **Stratification:** Applied for classification tasks to maintain class balance

### 5.3 Handling Class Imbalance (Classification Only)
For the bandgap type classification task (direct vs. indirect gap):
- Class imbalance assessed using class count ratios
- SMOTE (Synthetic Minority Over-sampling Technique) applied to training set if imbalance ratio > 1.5
- Test set remained unchanged to reflect true data distribution

## 6. Machine Learning Models

### 6.1 Primary Models

**Regression Task (Bandgap Prediction):**
- **LightGBM Regressor** (primary model)
  - Gradient boosting framework optimized for efficiency
  - Hyperparameters: n_estimators=1000, learning_rate=0.03, num_leaves=64, max_depth=-1
  - Early stopping with 50 rounds on validation set

**Classification Task (Gap Type):**
- **XGBoost Classifier** (primary model)
  - Extreme gradient boosting with regularization
  - Hyperparameters: n_estimators=500, learning_rate=0.05, max_depth=6

### 6.2 Baseline Models
For comparison, we trained:
- Random Forest (n_estimators=200)
- CatBoost (iterations=1000)
- Multilayer Perceptron (hidden layers: 100, 50)
- Support Vector Regressor (RBF kernel) / Logistic Regression

### 6.3 Hyperparameter Optimization
- **Method:** 5-fold cross-validation with grid search
- **Optimization metric:** Negative mean absolute error (regression), Accuracy (classification)
- **Search space:** See Supplementary Table S3 for complete parameter grids

All models were trained with random seed 42 to ensure reproducibility.

## 7. Model Evaluation

### 7.1 Regression Metrics
- **Mean Absolute Error (MAE):** Average absolute deviation
- **Root Mean Squared Error (RMSE):** Emphasizes larger errors
- **R² Score:** Coefficient of determination
- **Median Absolute Error:** Robust to outliers
- **Percentage of predictions with >25% error**

Evaluated using 5-fold cross-validation and hold-out test set.

### 7.2 Classification Metrics
- **Accuracy:** Overall correct predictions
- **Precision, Recall, F1-Score:** Per-class and weighted averages
- **ROC-AUC:** Area under receiver operating characteristic curve
- **Confusion Matrix:** Detailed error analysis

### 7.3 Statistical Comparison
Paired t-tests (or Wilcoxon signed-rank tests) were performed on cross-validation fold results to assess statistical significance of performance differences between models (α = 0.05).

## 8. Model Interpretability

### 8.1 Feature Importance
- Used built-in feature importance scores from tree-based models
- Ranked top 20 features by importance
- Compared with findings from Sradhasagar et al. (2024)

### 8.2 SHAP Analysis
- SHAP (SHapley Additive exPlanations) values computed for test set
- Summary plots showing global feature importance
- Dependence plots for top 3 features
- Force plots for individual material predictions

## 9. Candidate Material Selection

For photovoltaic applications, we searched for materials with:
- Predicted bandgap: 1.2 - 1.8 eV (optimal for solar cells)
- Energy above hull: < 0.2 eV/atom (synthesizable)
- Goldschmidt tolerance factor (τ) < 4.18 (Bartel criterion for perovskite formability)

The tolerance factor was computed using:
$$\\tau = \\frac{r_A + r_X}{\\sqrt{2}(r_B + r_X)} \\cdot \\frac{n_A}{n_A^\\text{ideal}}$$

where $r_A$, $r_B$, $r_X$ are ionic radii from Shannon tables, and $n_A$ is the coordination number.

## 10. Software and Reproducibility

### 10.1 Software Versions
- Python: 3.11+
- mp-api: 0.41.0
- scikit-learn: 1.3.0
- lightgbm: 4.0.0
- xgboost: 2.0.0
- matminer: 0.9.0
- pymatgen: 2023.9.0
- shap: 0.42.0

Complete package versions and system information are in Supplementary Table S4.

### 10.2 Reproducibility
- All random seeds set to 42
- Complete code repository: [GitHub URL]
- Preprocessing pipeline: Saved scalers and imputers
- Feature engineering: Documented feature generation
- Model checkpoints: Saved trained models with metadata

### 10.3 Data Availability
- Materials Project IDs: Supplementary Table S1
- Query parameters: experiments/query_config.yaml
- Processed features: Available upon request

## 11. Computational Resources
- **Platform:** Windows 11
- **Hardware:** Standard workstation (CPU-based training)
- **Training time:** Approximately 2-3 hours for complete pipeline (both F10 and F22 with SHAP analysis)
- **Parallelization:** Utilized all available CPU cores for tree-based models
- **Python environment:** Virtual environment (perovskite) with Python 3.11+

---
