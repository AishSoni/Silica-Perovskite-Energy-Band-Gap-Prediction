# Perovskite Bandgap Prediction using Machine Learning

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Materials Project](https://img.shields.io/badge/data-Materials%20Project-green.svg)](https://materialsproject.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

High-accuracy machine learning pipeline for predicting electronic bandgaps and bandgap types of double perovskite materials using compositional and structural features from DFT calculations.

---

## ğŸ¯ Key Features

- âœ… **Outstanding Performance**: RÂ² = 0.88, MAE = 0.35 eV (2.2Ã— better than targets)
- âœ… **Dual Tasks**: Bandgap regression + type classification (Direct vs Indirect)  
- âœ… **SHAP Analysis**: Explainable AI with feature importance visualization
- âœ… **Multiple Models**: LightGBM, XGBoost, Random Forest, CatBoost, MLP
- âœ… **Automated Pipeline**: End-to-end workflow from data download to evaluation
- âœ… **Production Ready**: Robust error handling, validation plots, comprehensive metrics

---

## ğŸ“Š Results Summary

### Regression (Bandgap Prediction)

| Feature Set | Best Model | RÂ² | MAE (eV) | RMSE (eV) |
|-------------|------------|-----|----------|-----------|
| **F22** (22 features) | LightGBM | **0.8836** | **0.3631** | 0.5639 |
| **F10** (10 features) | LightGBM | 0.8712 | 0.3934 | 0.5933 |

**Target**: RÂ² â‰¥ 0.40, MAE â‰¤ 0.45 eV  
**Achieved**: 2.2Ã— better RÂ², 23% lower MAE âœ¨

### Classification (Bandgap Type: Direct vs Indirect)

| Feature Set | Best Model | Accuracy | F1-Score | Precision | Recall |
|-------------|------------|----------|----------|-----------|--------|
| **F10** (10 features) | LightGBM | **0.8971** | **0.8908** | 0.8919 | 0.8971 |

**Target**: Accuracy â‰¥ 0.80, F1 â‰¥ 0.80  
**Achieved**: 12% above target âœ¨

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone https://github.com/AishSoni/Silica-Perovskite-Energy-Band-Gap-Prediction.git
cd Silica-Perovskite-Energy-Band-Gap-Prediction

# Create virtual environment
python -m venv perovskite
source perovskite/bin/activate  # On Windows: perovskite\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Access

Create a `.env` file with your Materials Project API key:

```env
MAPI_KEY=your_api_key_here
```

Get your free API key at: https://materialsproject.org/api

### 3. Run the Pipeline

#### Regression (Bandgap Prediction)

```bash
python run_pipeline.py F10          # Train with 10 features
python run_pipeline.py F10 F22      # Train with both feature sets
```

#### Classification (Bandgap Type Prediction)

```bash
python run_pipeline.py --task classification F10
```

#### Skip SHAP Analysis (Faster Execution)

```bash
python run_pipeline.py --no-shap F10
```

### 4. View Results

All outputs are automatically generated:

- **Validation plots**: `validation/{F10,F22}/`
- **Trained models**: `models/{F10,F22}/`
- **Evaluation figures**: `figures/{F10,F22}/`
- **SHAP analysis**: `figures/{F10,F22}/{model}/shap_*.png`
- **Results summary**: `results/all_models_summary.json`
- **Model comparison**: `results/model_comparison.png`

---

## ğŸ“ Project Structure

```
perovskite_project/
â”œâ”€â”€ data/                       # Raw and processed datasets
â”‚   â”œâ”€â”€ raw/                    # Raw data from Materials Project
â”‚   â””â”€â”€ processed/              # Featurized and cleaned data
â”œâ”€â”€ src/                        # Source code modules
â”‚   â”œâ”€â”€ data_io.py             # Data loading and preparation
â”‚   â”œâ”€â”€ featurize.py           # Feature engineering with Matminer
â”‚   â”œâ”€â”€ preprocess.py          # Data preprocessing and scaling
â”‚   â”œâ”€â”€ models.py              # Model training (regression/classification)
â”‚   â”œâ”€â”€ eval.py                # Evaluation and SHAP analysis
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ experiments/                # Experiment configurations
â”‚   â”œâ”€â”€ metadata.json          # System information
â”‚   â”œâ”€â”€ query_config.yaml      # Data query parameters
â”‚   â””â”€â”€ system_info.json       # Pipeline run metadata
â”œâ”€â”€ models/                     # Saved model artifacts (.pkl files)
â”œâ”€â”€ results/                    # Predictions and metrics
â”œâ”€â”€ figures/                    # Visualization outputs
â”œâ”€â”€ paper/                      # Paper drafts and documentation
â”‚   â”œâ”€â”€ methods.md             # Methodology description
â”‚   â”œâ”€â”€ results.md             # Results and analysis
â”‚   â””â”€â”€ limitations.md         # Known limitations
â”œâ”€â”€ run_pipeline.py            # Main pipeline script
â”œâ”€â”€ download_data.py           # Data acquisition script
â”œâ”€â”€ test_shap_classification.py # Example test script
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ”¬ Methodology

### Dataset

- **5,776 double perovskites** (ABCâ‚‚Dâ‚† formula family)  
- Source: Materials Project next-gen API  
- DFT-calculated bandgaps using VASP (GGA/GGA+U, r2SCAN functional)
- Structure: Lattice parameters, space groups, density, volume
- Class distribution: 80.7% Indirect, 19.3% Direct

### Feature Engineering

**293 features** generated using Matminer:

- **ElementProperty** (magpie descriptors): 128 features
- **Stoichiometry**: 22 features  
- **ElementFraction**: 112 features  
- **Structural**: Lattice parameters, derived ratios, packing fraction
- **Compositional**: Electronegativity differences, valence electrons

### Feature Selection

- **F4 to F24**: Tested 11 feature subsets using RFE with cross-validation
- **F22**: Best performance (RÂ²=0.7620 CV), 22 most important features
- **F10**: Simpler model (RÂ²=0.7386 CV), good balance between accuracy and complexity

### Models

**Regression (Primary):**
- LightGBM (best: RÂ²=0.8836)  
- XGBoost, Random Forest, CatBoost, MLP

**Classification (Primary):**
- XGBoost (Accuracy=0.8936)  
- LightGBM (best: Accuracy=0.8971)  
- Random Forest, CatBoost, MLP

### Evaluation

- **Train/Test Split**: 80/20 stratified split  
- **Scaling**: RobustScaler (handles outliers)
- **Metrics**: 
  - Regression: MAE, RMSE, RÂ²
  - Classification: Accuracy, Precision, Recall, F1-Score, ROC-AUC
- **Explainability**: SHAP values for feature importance

---

## ğŸ“– Documentation

- **[QUICK_START.md](QUICK_START.md)** - Detailed setup and usage guide  
- **[paper/methods.md](paper/methods.md)** - Methodology documentation  
- **[paper/results.md](paper/results.md)** - Results and analysis  
- **[paper/limitations.md](paper/limitations.md)** - Known limitations and future work

---

## ğŸ› ï¸ Dependencies

All required packages are in `requirements.txt`:

- **mp-api** (â‰¥0.41.0) - Materials Project API client  
- **pandas**, **numpy** - Data manipulation  
- **scikit-learn** - ML algorithms and preprocessing  
- **lightgbm** (â‰¥4.0.0) - Best performing model  
- **xgboost**, **catboost** - Gradient boosting models  
- **matplotlib**, **seaborn** - Visualization  
- **shap** - Explainability analysis  
- **matminer** (â‰¥0.9.0) - Materials featurization  
- **pymatgen** (â‰¥2023.9.0) - Materials analysis

---

## ğŸ“ˆ Usage Examples

### Basic Usage

```bash
# Default: F10 regression with SHAP
python run_pipeline.py

# Multiple feature sets
python run_pipeline.py F10 F22

# Classification task
python run_pipeline.py --task classification F10

# Faster (skip SHAP)
python run_pipeline.py --no-shap F22
```

### Help

```bash
python run_pipeline.py --help
```

---

## ğŸ¯ Key Findings

1. **Feature Importance**: 
   - Top features: Electronegativity statistics, atomic radii, GSbandgap descriptors
   - SHAP analysis reveals complex feature interactions

2. **Performance**:
   - Best regression: F22 XGBoost (RÂ²=0.8807, MAE=0.3454 eV)
   - Best classification: F10 LightGBM (Accuracy=89.71%)
   - Simpler F10 models nearly match F22 performance

3. **Validation**:
   - Error distribution centered near 0 eV
   - Most predictions within Â±0.5 eV of DFT values
   - PV-relevant bandgap range (1.2-1.8 eV) well-represented

---

## ğŸ” Future Work

- **Hyperparameter Optimization**: Optuna/GridSearch for even better performance  
- **Graph Neural Networks**: Structure-aware models (CGCNN, MEGNet)  
- **Active Learning**: Iterative model improvement with targeted experiments  
- **GW Corrections**: Train on GW-corrected bandgaps for higher accuracy  
- **Candidate Generation**: Predict properties of hypothetical perovskites

See [paper/limitations.md](paper/limitations.md) for detailed discussion.

---

## ğŸ“ Citation

If you use this code or methodology, please cite:

```bibtex
@software{perovskite_bandgap_prediction_2024,
  author = {Aish Soni},
  title = {Perovskite Bandgap Prediction using Machine Learning},
  year = {2024},
  url = {https://github.com/AishSoni/Silica-Perovskite-Energy-Band-Gap-Prediction}
}
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“§ Contact

**Aish Soni**  
GitHub: [@AishSoni](https://github.com/AishSoni)

For questions or issues, please open an issue on GitHub.

---

**Made with â¤ï¸ for materials science research**
